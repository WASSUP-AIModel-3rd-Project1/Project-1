{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. 각 metric_prjct 별로 데이터 통합, score 계산 결과 통합\n",
    "1. 각 metric_prjct 별로 mape, r2 score 값들의 분포 확인\n",
    "    - 각각에 대한 histogram & 2차원 histogram\n",
    "    - (각 metric_prjct 별로 튀는 부분에 대한 구체적 EDA) \n",
    "    - (각 metric_prjct 별로 scatter plot에서 데이터 hue e.g. 단위, 특징 등)\n",
    "    - 각 metric_prjct의 2차원 scatter plot을 한 곳에 그리기\n",
    "2. 각 item label 별로 prjct 별 score 비교\n",
    "    - mape, r2 score 각각\n",
    "    - (각각 별로 더 높은 prjct 표시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import missingno as msno\n",
    "import seaborn as sns \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools, itertools\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"png2x\")\n",
    "# 테마 설정: \"default\", \"classic\", \"dark_background\", \"fivethirtyeight\", \"seaborn\"\n",
    "mpl.style.use(\"fivethirtyeight\")\n",
    "# 이미지가 레이아웃 안으로 들어오도록 함\n",
    "mpl.rcParams.update({\"figure.constrained_layout.use\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "font_list = fm.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "[fm.FontProperties(fname=font).get_name() for font in font_list if 'D2C' in font]\n",
    "plt.rc('font', family='D2Coding')\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/home/doeun/code/AI/ESTSOFT2024/workspace/dataset/'\n",
    "load_dir = 'america_big_cities_health_inventory'\n",
    "file_name = 'BigCitiesHealth.csv'\n",
    "RSLT_DIR = '/home/doeun/code/AI/ESTSOFT2024/workspace/1.project1_structured/BCHI/processed/'\n",
    "pvtb_dir = RSLT_DIR + 'pvtb/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_key_opt(data:pd.DataFrame,key,opt):\n",
    "    rslt = dict()\n",
    "    labels = data[key].unique()\n",
    "    form = data[opt].value_counts().sort_values(ascending=False)\n",
    "    form.iloc[:] = 0\n",
    "\n",
    "    for feat in labels:\n",
    "        cond = data[key]==feat\n",
    "        val = form.copy()\n",
    "        temp = data.loc[cond,opt].value_counts()\n",
    "        val.loc[temp.index] = temp\n",
    "        rslt[feat] = val\n",
    "\n",
    "    return pd.DataFrame(rslt).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def cond_check_dict(data=pd.DataFrame,val_dict=dict):\n",
    "    cond_list=[\n",
    "        data[col] == val\n",
    "        for col, val in val_dict.items()\n",
    "    ]\n",
    "    return functools.reduce(lambda x,y: x & y, cond_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Racial\n",
      "Segregation\n",
      "Indices |\n",
      "Racial\n",
      "Segregation,\n",
      "White and\n",
      "Hispanic\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def choose_split_point(word_len,space,ths):\n",
    "    # 윗 줄에 space 만큼 공백이 있고, 한 줄의 길이가 ths로 제한 되어있을 때\n",
    "    # 어떤 지점에서 단어를 끊어줄지 정하기\n",
    "    # |-------ths-------|\n",
    "    # |-space-|---------|-space-|------| : word\n",
    "    #         |-------ths-------|\n",
    "    print(word_len,space,ths)\n",
    "    if word_len < ths + space :\n",
    "        if abs(word_len/2 -ths) <= abs(word_len/2-space) :\n",
    "            return word_len-ths\n",
    "        else :\n",
    "            return word_len - space if word_len < 2 * space else space\n",
    "    else :\n",
    "        return ths if word_len - (ths + space) < 0.3 * ths else space\n",
    "\n",
    "def minimize_seq_idx_np(domain:np.array,func):\n",
    "    vfunc = np.vectorize(func)\n",
    "    temp = np.argsort(vfunc(domain))\n",
    "    return temp[0]\n",
    "\n",
    "def modify_strlen_ths(last,new,ths=16):\n",
    "    front = len(last)\n",
    "    space = ths - (1+front)\n",
    "    if len(new) < space :\n",
    "        rslt = [last + ' ' + new]\n",
    "    else :\n",
    "        if len(new) < ths:\n",
    "            rslt = [last, new]\n",
    "        else:\n",
    "            cut = choose_split_point(len(new),space-1,ths-1)\n",
    "            new_h, new_e = new[:cut]+'-', new[cut:]\n",
    "            if cut < ths-1 :\n",
    "                rslt = modify_strlen_ths(last+' '+new_h,new_e)\n",
    "            else :\n",
    "                rslt = [last] + modify_strlen_ths(new_h,new_e) \n",
    "    return rslt\n",
    "\n",
    "def str_cutter(sentnc, ths = 16):\n",
    "    words= sentnc.split(' ')\n",
    "    rslt, pnt = [''], 0\n",
    "    while pnt < len(words):\n",
    "        last = '' if len(rslt)==0 else rslt[-1]\n",
    "        next_ele = modify_strlen_ths(last,words[pnt],ths)\n",
    "        rslt = rslt[:-1] + next_ele\n",
    "        pnt += 1\n",
    "    return '\\n'.join(rslt)[1:]\n",
    "#알고리즘 때문에 맨 앞에 빈칸 하나 들어가게 되는 이슈 있음\n",
    "\n",
    "print(str_cutter('Racial Segregation Indices | Racial Segregation, White and Hispanic', 13))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_plot_grid(n:int,r_max=8,c_max=17,res_ths=2):\n",
    "    #ver2\n",
    "    r_min = np.ceil(n/c_max)\n",
    "    sppt = np.arange(r_min,r_max+1) #need error process\n",
    "    col_nums = np.ceil(n/sppt)\n",
    "    res = col_nums * sppt -n\n",
    "    min_idx = np.where((res==np.min(res)) | (res <= res_ths))[0]\n",
    "    row_cand, col_cand = sppt[min_idx], col_nums[min_idx]\n",
    "    if len(min_idx) > 1 :\n",
    "        res = np.abs(row_cand-col_cand)\n",
    "        i = np.where(res==np.min(res))[0][0]\n",
    "    else : i = 0\n",
    "    return int(row_cand[i]), int(col_cand[i])\n",
    "\n",
    "def pair_plot_feat_hue(fig,axes,data:dict,pair_plot,axis_share=False,hue_label_dict=None, **kwargs):\n",
    "    #ver2\n",
    "    if (fig is None) or (axes is None) :\n",
    "        num_r, num_c = choose_plot_grid(len(data))\n",
    "        fig, axes = plt.subplots(num_r,num_c,figsize=(4*num_c,4*num_r),sharex=axis_share,sharey=axis_share)\n",
    "    for n,key in enumerate(data.keys()):\n",
    "        ax = axes.flatten()[n]\n",
    "        plt.setp(ax.get_xticklabels(),ha = 'left',rotation = 90)\n",
    "        if n >= len(data) : continue\n",
    "        pair_plot(x=data[key][0], y = data[key][1],ax =ax, **kwargs)\n",
    "        feat_name = str(key) \n",
    "        if hue_label_dict: color = 'b' if hue_label_dict[feat_name] else 'k'\n",
    "        else : color = 'k'\n",
    "        ax.set_xlabel(str_cutter(feat_name,20),loc='left',fontsize = 8.3,color=color)\n",
    "    return fig,axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS - DATA- SCORE REG RSLT\n",
    "\n",
    "def make_reg_score_dict(y_actual,y_pred,base_val):\n",
    "    rmse_model, rmse_base = np.sqrt(mse(y_actual,y_pred)), np.sqrt(mse([base_val]*len(y_actual),y_actual))\n",
    "    mape_model, mape_base = mape(y_actual,y_pred), mape([base_val]*len(y_actual),y_actual)\n",
    "    r2_model, r2_base = r2_score(y_actual,y_pred), 0\n",
    "    \n",
    "    return {\n",
    "        'rmse' : [rmse_model, rmse_base],\n",
    "        'mape' : [mape_model, mape_base],\n",
    "        'r2_score' : [r2_model,r2_base]\n",
    "    }\n",
    "\n",
    "def print_reg_score_dict(name,dict_score):\n",
    "    print('{}\\nr2 score : {:.5f}'.format(name,dict_score['r2_score'][0]))\n",
    "    print('rmse_model : {:.5f} / rmse_base : {:.5f}\\t'.format(*dict_score['rmse']),\n",
    "          'mape_model : {:.5f} / mape_base : {:.5f}\\t'.format(*dict_score['mape']))\n",
    "\n",
    "def make_reg_score_dict_cols(target_sample,dict_df,dict_train_test,dict_rslt,print_plot=False):\n",
    "    dict_score = dict()\n",
    "    for col in target_sample:\n",
    "        train_y = dict_df[col]['train'][1]\n",
    "        valid_y = dict_train_test[col][3]\n",
    "        y_pred = dict_rslt[col]['valid']\n",
    "        dict_score[col] = make_reg_score_dict(valid_y,y_pred,np.mean(train_y))\n",
    "        if print_plot :\n",
    "            print_reg_score_dict(col,dict_score[col])\n",
    "            print('-'*150)\n",
    "    return dict_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS - DATA- PLOT REG RSLT\n",
    "from itertools import repeat, chain\n",
    "\n",
    "def scatter_reg_rslt(dict_train_test,dict_rslt,dict_score): #set_iput\n",
    "    target_sample = list(dict_rslt.keys())\n",
    "    data_plot ={\n",
    "        col : (dict_train_test[col][3], dict_rslt[col]['valid'])\n",
    "        for col in target_sample \n",
    "    }\n",
    "    data_line = {\n",
    "        col : (dict_train_test[col][3],dict_train_test[col][3])\n",
    "        for col in target_sample \n",
    "    }\n",
    "#    fig,axes = plt.subplots(3,3,figsize=(12,12))\n",
    "#    fig,axes = pair_plot_feat_hue(fig=fig,axes=axes,data=data_line,\n",
    "    fig,axes = pair_plot_feat_hue(fig=None,axes=None,data=data_line,\n",
    "                                  pair_plot=sns.lineplot,lw=0.3)\n",
    "    #fig.set_size_inches(12,8, forward=True)\n",
    "    fig,axes = pair_plot_feat_hue(fig=fig,axes=axes,data=data_plot,\n",
    "                                  pair_plot=sns.scatterplot,s=5,alpha=0.65)\n",
    "\n",
    "    for n,key in enumerate(data_plot.keys()):\n",
    "        ax = axes.flatten()[n]\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_title(key,fontsize=12)\n",
    "        ax.set_xlabel('rmse_model : {:.3f} | rmse_base : {:.3f}\\nr2 score : {:.3f} {:>35}\\n'.format(*dict_score[key]['rmse'],dict_score[key]['r2_score'][0],\n",
    "            f'n = {len(dict_train_test[key][0])}'), fontsize=10, ha ='left')\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        plt.setp(ax.get_yticklabels(),rotation = 0, fontsize = 9)\n",
    "        plt.setp(ax.get_xticklabels(),ha ='center',rotation = 0, fontsize = 9)\n",
    "    \n",
    "    return fig,axes\n",
    "\n",
    "def plot_reg_score(dict_train_test,dict_rslt,dict_score,target_sample):\n",
    "    data_plot ={\n",
    "        col : (dict_train_test[col][3], dict_rslt[col]['valid'])\n",
    "        for col in target_sample \n",
    "    }\n",
    "    fig,axes = plt.subplots(len(data_plot),3,figsize=(15,20))\n",
    "    for n, col in enumerate(data_plot.keys()):\n",
    "        #quo, rem = divmod(n,3)\n",
    "        ax1, ax2, ax3 = axes[n][0], axes[n][1], axes[n][2]\n",
    "        test_y, y_pred = data_plot[col]\n",
    "        train_y = dict_train_test[col][2]\n",
    "\n",
    "        sns.histplot(test_y,label='actual',ax=ax1,alpha=0.5)\n",
    "        sns.histplot(y_pred,label='knn_pred',ax=ax1,alpha=0.5)\n",
    "        ax1.legend(fontsize=9)\n",
    "\n",
    "        sns.histplot(test_y-np.mean(train_y),ax=ax2, label = 'baseline',alpha=0.5)\n",
    "        sns.histplot(test_y-y_pred,ax=ax2, label = 'knn_pred',alpha=0.5)\n",
    "        ax2.legend(fontsize=9)\n",
    "\n",
    "        df_score = pd.DataFrame(dict_score[col]).T[[1,0]]\n",
    "        xs = list(chain.from_iterable(repeat(val,2) for val in df_score.index))\n",
    "        ax3r = ax3.twinx()\n",
    "        sns.barplot(x=xs[:2],y=list(df_score.values.reshape(-1))[:2],\n",
    "                    hue = ['knn_pred','base']*1,ax=ax3,alpha=0.65,legend=False)\n",
    "        sns.barplot(x=xs[2:],y=list(df_score.values.reshape(-1))[2:],\n",
    "                    hue = ['knn_pred','base']*2,ax=ax3r,alpha=0.8,legend=False)\n",
    "        ax3.set_yscale('log')\n",
    "        ax3r.set_ylim([0.0,1.15])\n",
    "        ax3r.bar_label(ax3r.containers[0], fontsize=8, fmt='%.4f')\n",
    "        ax3r.bar_label(ax3r.containers[1], fontsize=8, fmt='%.4f')\n",
    "        ax3.grid(False)\n",
    "        ax3r.grid(False)\n",
    "        ax3r.set_yscale('linear')    \n",
    "        ax3.bar_label(ax3.containers[0], fontsize=8, fmt='%.4f')\n",
    "        ax3.bar_label(ax3.containers[1], fontsize=8, fmt='%.4f')\n",
    "        #ax3.axvline()\n",
    "\n",
    "        ax1.set_title(str_cutter(col,50),fontsize=15,loc='left',ha='left')\n",
    "        ax2.xaxis.set_label_coords(-0.02, -0.15)\n",
    "        ax2.set_xlabel('rmse_model : {:.3f} | rmse_base : {:.3f}\\nr2 score : {:.3f} {:>48}\\n'.format(*dict_score[col]['rmse'],dict_score[col]['r2_score'][0],\n",
    "            f'n = {len(dict_train_test[col][0])}'), fontsize=10,ha ='left')\n",
    "        #ax2.xaxis.set_label_position('left')\n",
    "        ax1.set_xlabel('')\n",
    "        ax3.set_xlabel('')\n",
    "        ax1.set_ylabel('count',fontsize =10) \n",
    "        ax2.set_ylabel('count',fontsize =10) \n",
    "        plt.setp(ax3.get_yticklabels(),rotation = 0, fontsize = 9, color='#333333')\n",
    "        plt.setp(ax3r.get_yticklabels(),rotation = 0, fontsize = 9)\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        plt.setp(ax.get_yticklabels(),rotation = 0, fontsize = 9)\n",
    "        plt.setp(ax.get_xticklabels(),ha ='center',rotation = 0, fontsize = 9)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS - FILE I/O\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_pkl(save_dir,file_name,save_object):\n",
    "    if not os.path.exists(save_dir): os.mkdir(save_dir)\n",
    "    file_path = os.path.join(save_dir,file_name)\n",
    "    with open(file_path,'wb') as f:\n",
    "        pickle.dump(save_object,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_canberra7_cand',\n",
       " 'knn_canberra5_else',\n",
       " 'knn_custom7_cand',\n",
       " 'knn_braycurtis5_cand',\n",
       " 'knn_cand',\n",
       " 'pivot_racesex.csv',\n",
       " 'knn_braycurtis7_cand',\n",
       " 'geo_strat_info.csv',\n",
       " 'knn_braycurtis7_else',\n",
       " 'knn_custom5_cand',\n",
       " 'knn_custom7_else',\n",
       " 'knn_canberra5_cand',\n",
       " 'knn_braycurtis5_else',\n",
       " 'pvtb',\n",
       " 'knn_custom5_else',\n",
       " 'knn_cosine5_cand',\n",
       " 'knn_canberra7_else',\n",
       " 'group_statistics.csv',\n",
       " 'knn_else']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(RSLT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(filter(lambda x : ('knn' in x) & ('5' in x) ,os.listdir(RSLT_DIR)))\n",
    "file_dict = dict()\n",
    "for filename in files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_path = os.path.join(RSLT_DIR,knn_dir)\n",
    "work_idx = 0\n",
    "n_work = 5 \n",
    "\n",
    "for work_idx in tqdm(range(1,5)):\n",
    "\n",
    "    work_name = '{}_{}'.format(work_idx, n_work)\n",
    "    target_sample = target_cols[work_idx::n_work]\n",
    "    work_name = '{}_{}'.format(work_idx,n_work) \n",
    "    print(f'work : {work_name}')\n",
    "\n",
    "    pvtb_encoded['city_idx'] = pvtb_encoded['geo_label_city'].apply(lambda x : city_list.index(x))\n",
    "    temp = list(pvtb_encoded.columns)\n",
    "    pvtb_encoded_whole = pvtb_encoded[['city_idx']+temp[1:-1]]\n",
    "    test_df = pvtb_encoded[info_cols+target_cols]\n",
    "\n",
    "\n",
    "    files = list(filter(lambda x : work_name in x,os.listdir(knn_path)))\n",
    "    file_dict = dict()\n",
    "    for filename in files:\n",
    "        if filename[-4:] != '.pkl' : continue\n",
    "        if 'knn' in filename : continue\n",
    "        print(filename)\n",
    "        file_path = os.path.join(knn_path,filename)\n",
    "        with open (file_path,'rb') as f:\n",
    "            file_dict[filename[:-5-len(work_name)]] = pickle.load(f)\n",
    "\n",
    "    dict_rslt = file_dict['dict_rslt']\n",
    "    target_sample= list(dict_rslt.keys())\n",
    "\n",
    "    rslt_form = test_df[info_cols+target_sample]\n",
    "\n",
    "    for col in target_sample:\n",
    "        cond = test_df[col].isna()\n",
    "        loaded = dict_rslt[col]['target']\n",
    "        assert np.sum(cond) == len(loaded)  \n",
    "        rslt_form.loc[cond,col] = loaded\n",
    "\n",
    "    file_name = 'pvtb_filled_knn_{}_{}.csv'.format(work_idx,10)\n",
    "    save_dir = os.path.join(RSLT_DIR,knn_dir,'PROCESSED')\n",
    "    if not os.path.exists(save_dir): os.mkdir(save_dir)\n",
    "    rslt_form.to_csv(os.path.join(save_dir,file_name))\n",
    "\n",
    "    ### check score for loaded data\n",
    "\n",
    "    dict_rslt=file_dict['dict_rslt']\n",
    "    dict_train_test=file_dict['dict_train_test']\n",
    "\n",
    "    dict_df = dict()\n",
    "    for col in target_sample:\n",
    "        temp = test_df[info_cols+[col]]\n",
    "        cond_na = temp.isna().any(axis=1)\n",
    "        dict_df[col] = {\n",
    "            'train' : [temp.loc[~cond_na,info_cols], temp.loc[~cond_na,col]],\n",
    "            'target' : [temp.loc[cond_na,info_cols], cond_na],\n",
    "        }\n",
    "\n",
    "    ## check score\n",
    "    dict_score = make_reg_score_dict_cols(target_sample,dict_df,dict_train_test,dict_rslt,print_plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EST_NA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
